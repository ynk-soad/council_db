import os
import re
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin

# è¨­å®š
BASE_URL = "https://www.city.mima.lg.jp"
LIST_PAGE = BASE_URL + "/gyosei/shisei/gikai/kaigiroku/"
SAVE_DIR = "/Users/ynkhiru09/Library/CloudStorage/OneDrive-KansaiUniversity/å››å›½/å¾³å³¶çœŒ/mima"
JIS_CODE = "36207"
os.makedirs(SAVE_DIR, exist_ok=True)

# å’Œæš¦â†’è¥¿æš¦å¤‰æ›
def convert_japanese_date(text):
    match = re.search(r'(ä»¤å’Œ|å¹³æˆ)(å…ƒ|\d+)å¹´(?:ç¬¬\d+å›)?(?:å®šä¾‹ä¼š|è‡¨æ™‚ä¼š)?.*?(\d{1,2})æœˆ(\d{1,2})æ—¥', text)
    if not match:
        return None
    era, year_str, month, day = match.groups()
    base_year = 2018 if era == 'ä»¤å’Œ' else 1988
    year = base_year + (1 if year_str == 'å…ƒ' else int(year_str))
    return f"{year}{int(month):02}{int(day):02}"

# å¹´åˆ¥ãƒšãƒ¼ã‚¸ãƒªãƒ³ã‚¯å–å¾—
res = requests.get(LIST_PAGE)
soup = BeautifulSoup(res.text, "html.parser")
year_links = soup.select("article header h2 a")

for link in year_links:
    href = link.get("href")
    year_page_url = urljoin(BASE_URL, href)
    print(f"ğŸ“„ å¹´åˆ¥ãƒšãƒ¼ã‚¸: {year_page_url}")

    res_year = requests.get(year_page_url)
    res_year.encoding = res_year.apparent_encoding  
    soup_year = BeautifulSoup(res_year.text, "html.parser")

    for a in soup_year.find_all("a", href=True):
        if ".pdf" in a["href"].lower():
            pdf_url = urljoin(BASE_URL, a["href"])
            text = a.text.strip()
            date_str = convert_japanese_date(text)
            if date_str:
                year = int(date_str[:4])
                if year < 2011:
                    print(f"â­ {text} ã¯ {year} å¹´ãªã®ã§ã‚¹ã‚­ãƒƒãƒ—")
                    continue
                filename = f"{date_str}{JIS_CODE}.pdf"
            else:
                print(f"âš  æ—¥ä»˜å¤‰æ›å¤±æ•—: {text}")
                continue

            save_path = os.path.join(SAVE_DIR, filename)
            print(f"â¬‡ {filename} ã‚’ä¿å­˜ä¸­...")

            try:
                pdf_res = requests.get(pdf_url)
                with open(save_path, "wb") as f:
                    f.write(pdf_res.content)
            except Exception as e:
                print(f"âŒ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {pdf_url} ({e})")

print("âœ… å…¨PDFã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
